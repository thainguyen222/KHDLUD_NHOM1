{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thainguyen222/KHDLUD_NHOM1/blob/main/Report_KHDLUD_NHOM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk6wgYYFG1bS"
      },
      "source": [
        "# Báo cáo đồ án môn \"Khoa học dữ liệu ứng dụng\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klnVHLYiLZMz"
      },
      "source": [
        "Nhóm 1:\n",
        "1. 1612571 - Trần Tiến Sỹ - AllenAlexander98\n",
        "2. 1612607 - Nguyễn Quang Thái - thainguyen222\n",
        "3. 18120228\t- Huỳnh Nhựt Quang - huynhnhutquang7\n",
        "\n",
        "Link thùng chứa Github của nhóm: https://github.com/thainguyen222/KHDLUD_NHOM1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mô tả bài toán"
      ],
      "metadata": {
        "id": "ugLuVxntXHiq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Giới Thiệu Bài Toán**\n",
        "\n",
        "  Bài toán dự đoán về lượng mưa ở các bang trung và tây Mỹ vào tháng tư đến tháng tám năm 2014, dựa vào số liệu ghi lại từ radar phân cực.\n",
        "  \n",
        "https://www.kaggle.com/competitions/how-much-did-it-rain-ii/\n",
        "\n",
        "  Lượng mưa luôn là một hiện tượng thay đổi theo không gian và thời gian, cho nên việc đo lường lượng mưa cũng nổi tiếng là khó khăn để đo chính xác. Đồng hồ đo mưa là một công cụ có thể đo chính xác và hiệu quả , tuy nhiên ta lại không đặt chúng ở mọi nơi. Vì vậy, để bao phủ một phạm vi rộng lớn , người ta hay dùng đến radar thời tiết để dự báo, tuy nhiên sai lệch giữa dự báo dựa trên radar thời tiết và thực tế khá cao.\n",
        "Gần đây, để cải thiện khả năng dự báo lượng mưa, the U.S National Weather Service đã thay thế các radar mạng lưới thành các radar phân cực. Điều này tạo nên độ chính xác cao hơn cho việc dự báo lượng mưa.\n",
        "Trong cuộc thi này, người tham gia sẽ nhận được số liệu từ các radar mạng phân cực (22 features), từ đó dự đoán ra tổng lượng mưa theo giờ."
      ],
      "metadata": {
        "id": "IVEQvTk1XKsR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mô tả dữ liệu\n"
      ],
      "metadata": {
        "id": "QxrPUqC5XPWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Dữ liệu training đào tạo bao gồm dữ liệu NEXRAD và MADIS thu thập vào 20 ngày giữa tháng Tư và tháng Tám của năm 2014 tại các bang trồng ngô miền trung và tây nước Mỹ. Thông tin về thời gian và địa điểm đã được kiểm duyệt, và được xáo trộn để chúng không được sắp xếp theo thứ tự thời gian và địa điểm. Dữ liệu test gồm dữ liệu từ cùng 1 radar và máy đo trong những ngày còn lại trong tháng.\n",
        "\n",
        "**Training dataset**\n",
        "\n",
        "Các thông số được ghi lại tại các radar và máy đo ở Trung Tây Hoa Kỳ trong 20 ngày mỗi tháng trong mùa trồng ngô (tháng tư đến tháng tám), các thông số ghi lại tại cuối mỗi giờ. \n",
        "\n",
        "**Test dataset**\n",
        "\n",
        "Tương tự như Training dataset, các thông số được ghi lại tại các radar và máy đo ở Trung Tây Hoa Kỳ trong 10 hoặc 11 ngày còn lại mỗi tháng trong mùa trồng ngô (tháng tư đến tháng tám) tương ứng với Training dataset. Ta cần dự đoán thông số của máy quan sát radar tại cuối mỗi giờ (tính bằng mm).\n",
        "\n",
        "**Đánh giá**\n",
        "Ở cuộc thi này, người ta dùng độ đo Mean Absolute Error (MAE).\n"
      ],
      "metadata": {
        "id": "rHkwORRyXU0J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mô tả file "
      ],
      "metadata": {
        "id": "0cCVovqYXgiZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cần chú ý, có nhiều máy đo trong suốt một giờ tại một điểm, và chỉ có một máy đo ở cuối giờ được tính ( cột the Expected ) được tính. Vì vậy, sẽ có nhiều dòng trùng \"Id\" với nhau\n",
        "\n",
        "Các cột trong dữ liệu:\n",
        "- Id: là số để phân biệt từng bộ quan sát suốt một giờ, tại một máy đo.\n",
        "- minutes_past: đối với một bộ quan sát radar, minutes_past là thời gian mà radar đó đã quan sát tính theo phút từ đầu giờ. \n",
        "- radardist_km: Khoảng cách của máy đo từ radar có các quan sát đang được báo cáo.\n",
        "- Ref:  Hệ số phản xạ của radar tính bằng km.\n",
        "- Ref_5x5_10th: Phần trăm giá trị hệ số phản xạ thứ 10 trong vùng lân cận 5x5 xung quanh thước đo.\n",
        "- Ref_5x5_50th: Phần trăm giá trị hệ số phản xạ thứ 50 trong vùng lân cận 5x5 xung quanh thước đo.\n",
        "- Ref_5x5_90th: Phần trăm giá trị hệ số phản xạ thứ 90 trong vùng lân cận 5x5 xung quanh thước đo.\n",
        "- RefComposite: Hệ số phản xạ tối đa trong cột dọc phía trên thước đo. Tính bằng dBZ.\n",
        "- RefComposite_5x5_10th\n",
        "- RefComposite_5x5_50th\n",
        "- RefComposite_5x5_90th\n",
        "- RhoHV: Hệ số tương quan (không đơn vị) \n",
        "- RhoHV_5x5_10th\n",
        "- RhoHV_5x5_50th\n",
        "- RhoHV_5x5_90th\n",
        "- Zdr: Hệ số phản xạ vi sai tính bằng dB\n",
        "- Zdr_5x5_10th\n",
        "- Zdr_5x5_50th\n",
        "- Zdr_5x5_90th\n",
        "- Kdp: Pha vi sai cụ thể (độ/km)\n",
        "- Kdp_5x5_10th\n",
        "- Kdp_5x5_50th\n",
        "- Kdp_5x5_90th"
      ],
      "metadata": {
        "id": "VK4H9QPOXieZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Solution: Recurrent Neural Networks"
      ],
      "metadata": {
        "id": "Bihjyz0Qvoi2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dự đoán về các giá trị tích lũy từ các chuỗi vectơ có độ dài thay đổi với yếu tố 'thời gian' rất gợi nhớ đến cái gọi là Adding Problem trong Machine Learning — một nhiệm vụ hồi quy trình tự đồ chơi được thiết kế để chứng minh sức mạnh của mạng nơ-ron tuần hoàn (RNN ) trong việc học các phụ thuộc dài hạn:\n",
        "\n",
        "![RNN_adding.png](data:image/png;base64,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)\n",
        "\n",
        "The prediction target là 1,7 có được bằng cách cộng các số ở hàng trên cùng với giá trị tương ứng ở hàng dưới cùng bằng một (tức là các ô màu xanh lá cây). Nhiệm vụ hồi quy là suy ra mô hình tổng hợp này từ một tập hợp các chuỗi ngẫu nhiên có độ dài tùy ý và mục tiêu của chúng.\n",
        "\n",
        "Trong bài toán dự đoán lượng mưa của chúng em, có bước bổ sung là suy ra the rainfall ‘numbers’ (hàng trên cùng) từ các phép đo radar. Hơn nữa, thay vì các giá trị nhị phân 0/1 (hàng dưới cùng), một giá trị có thời gian đọc liên tục từ 0 đến 60 phút có vai trò hơi khác nhau. Tuy nhiên, những điểm tương đồng về cấu trúc cơ bản đủ thuyết phục để gợi ý rằng Recurrent Neural Networks rất phù hợp để giải quyết vấn đề.\n",
        "\n",
        "Để có cái nhìn tổng quan về Recurrent Neural Networks, bài đăng trên blog của [Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) là một bài giới thiệu tổng quát về chủ đề này mà bạn sẽ tìm thấy ở bất cứ đâu."
      ],
      "metadata": {
        "id": "fCqTmUkUwqpE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Data"
      ],
      "metadata": {
        "id": "IY9AWuNENP2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pre-processing\n",
        "Ngay từ đầu, đã có một số thách thức do một số đặc điểm của dữ liệu mang lại:\n",
        "\n",
        "1. Các ngoại lệ cực đoan và không thể đoán trước\n",
        "\n",
        "2. Độ dài trình tự thay đổi và thời gian đo radar không đều\n",
        "\n",
        "3. Tập huấn luyện với các mẫu được phân phối không độc lập\n",
        "\n",
        "# Các ngoại lệ cực đoan và không thể đoán trước\n",
        "Nó đã được ghi lại đầy đủ ngay từ đầu và được thảo luận nhiều trong diễn đàn cạnh tranh, rằng một tỷ lệ lớn các phép đo mưa hàng giờ không được tin cậy (ví dụ: máy đo mưa bị tắc). Do một số giá trị này cao hơn vài bậc so với mức độ có thể thực hiện được ở bất kỳ đâu trên trái đất, các giá trị MAE mà những người tham gia báo cáo đã bị chi phối bởi những giá trị ngoại lai cực đoan này. Tuy nhiên, vì chỉ số đánh giá là sai số trung bình tuyệt đối (MAE) chứ không phải là sai số bình phương gốc (RMSE), nên người ta có thể đơn giản xem các giá trị ngoại lai như một nguồn gây nhiễu bên ngoài.\n",
        "\n",
        "Cách tiếp cận mà tác giả và nhiều người khác thực hiện chỉ đơn giản là loại trừ khỏi tập train các đồng hồ đo mưa có chỉ số trên 70mm. Trong suốt cuộc thi, tác giả đã thử nghiệm với một số ngưỡng khác nhau từ 53mm đến 73mm và thực hiện một vài lần chạy trong đó tác giả đã loại bỏ hoàn toàn bước tiền xử lý này. Trái ngược với những gì đã được báo cáo trong phiên bản trước của cuộc thi này, điều này có rất ít ảnh hưởng đến hiệu suất của mô hình (tích cực hoặc tiêu cực). Tác giả suy đoán rằng các mô hình RNN đã học cách bỏ qua các ngoại lệ, như được đề xuất bởi các giá trị tối đa rất hợp lý của các mức đo mưa hàng giờ dự kiến được dự đoán cho tập test (~ 45-55mm).\n",
        "\n",
        "# Độ dài trình tự thay đổi và thời gian đo radar không đều\n",
        "Các chuỗi radar thời tiết có độ dài khác nhau, từ 1 đến 19 lần đọc cho mỗi bản ghi đo mưa hàng giờ. Hơn nữa, những lần đọc này được thực hiện tại các điểm dường như ngẫu nhiên trong giờ. Nói cách khác, đây không phải là tập dữ liệu chuỗi thời gian điển hình (giá thị trường chứng khoán, v.v.).\n",
        "\n",
        "Một tính năng hấp dẫn của RNN là chúng chấp nhận các chuỗi đầu vào có độ dài khác nhau do chia sẻ trọng lượng (weight sharing) trong các hidden layer. Bởi vì điều này, tác giả đã không thực hiện bất kỳ xử lý trước nào ngoài việc loại bỏ các ngoại lai (như mô tả ở trên) và thay thế bất kỳ giá trị tính năng radar bị thiếu nào bằng 0. Tác giả đã giữ lại từng mốc thời gian như một thành phần trong vectơ đặc trưng và giữ nguyên bản chất tuần tự của đầu vào.\n",
        "\n",
        "# Tập huấn luyện với các mẫu được phân phối không độc lập\n",
        "Tập train bao gồm dữ liệu từ 20 ngày đầu tiên của mỗi tháng và dữ liệu tập test từ những ngày còn lại. Điều này đảm bảo rằng cả hai tập hợp ít nhiều độc lập. Tuy nhiên, như đã được chỉ ra trong diễn đàn cạnh tranh, do thông tin về thời gian và địa điểm bị bỏ qua, nên không thể tạo tập con tài trợ xác thực cục bộ thực sự độc lập với phần còn lại của tập huấn luyện. Ngoài ra, không có cách nào để đảm bảo rằng hai đồng hồ đo bất kì là không có mối liên hệ về thời gian hoặc không gian. Điều này cho thấy rằng rất khó phát hiện các trường hợp overfitting mà không nộp bảng xếp hạng công khai."
      ],
      "metadata": {
        "id": "D4HkxNXeNUTQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOURCE CODE"
      ],
      "metadata": {
        "id": "2d3kS7UfFhYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## environment\n"
      ],
      "metadata": {
        "id": "xLCBfaLoZKOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pandas  > /dev/null\n",
        "#!pip install numpy > /dev/null\n",
        "#!pip install sklearn > /dev/null\n",
        "#!pip install theano > /dev/null\n",
        "#!pip install lasagne > /dev/null\n",
        "#!pip install --upgrade https://github.com/Theano/Theano/archive/master.zip > /dev/null\n",
        "!pip install --upgrade https://github.com/Lasagne/Lasagne/archive/master.zip > /dev/null\n",
        "\n"
      ],
      "metadata": {
        "id": "78C5oCtAX4s8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r KHDLUD_NHOM1\n",
        "!rm -r data\n",
        "!git clone https://github.com/thainguyen222/KHDLUD_NHOM1.git\n",
        "!mkdir data\n",
        "!cp KHDLUD_NHOM1/train.csv data/\n",
        "!cp KHDLUD_NHOM1/test.csv data/\n",
        "!cp KHDLUD_NHOM1/NN_architectures.py /usr/lib/python3.7/\n",
        "!unzip KHDLUD_NHOM1/Theano-master.zip > /dev/null\n",
        "!rm -r /usr/local/lib/python3.7/dist-packages/theano\n",
        "!mv Theano-master/theano /usr/local/lib/python3.7/dist-packages"
      ],
      "metadata": {
        "id": "_c0carbjuj7I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4ee0af0-f2e1-4404-e2fa-c73ba9b9bef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove 'KHDLUD_NHOM1': No such file or directory\n",
            "rm: cannot remove 'data': No such file or directory\n",
            "Cloning into 'KHDLUD_NHOM1'...\n",
            "remote: Enumerating objects: 110, done.\u001b[K\n",
            "remote: Counting objects: 100% (19/19), done.\u001b[K\n",
            "remote: Compressing objects: 100% (19/19), done.\u001b[K\n",
            "remote: Total 110 (delta 6), reused 0 (delta 0), pack-reused 91\u001b[K\n",
            "Receiving objects: 100% (110/110), 17.26 MiB | 9.86 MiB/s, done.\n",
            "Resolving deltas: 100% (38/38), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!ls /usr/local/lib/python3.7/dist-packages/theano/sandbox/\n",
        "#!cat /usr/local/lib/python3.7/dist-packages/theano/sandbox/rng_mrg.py | grep MRG_RandomStreams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpbXObj1ww_Q",
        "outputId": "a4d666f4-8849-46b6-f18b-f2579dc22030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blocksparse.py\t__init__.py\tneighbours.py\t\t     solve.py\n",
            "conv.py\t\tlinalg\t\trng_mrg.py\t\t     tests\n",
            "cuda\t\tminimal.py\tsamples_MRG31k3p_12_7_5.txt\n",
            "fourier.py\tmultinomial.py\tsoftsign.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import gc\n",
        "import sys\n",
        "import time\n",
        "import lasagne\n",
        "import lasagne.layers as LL\n",
        "from lasagne.objectives import aggregate\n",
        "from lasagne.random import set_rng #, get_rng\n",
        "import theano\n",
        "#import theano.tensor as T\n",
        "from theano import tensor as T\n",
        "from NN_architectures import build_1Dregression_v1\n",
        "from NN_architectures import build_1Dregression_v2\n",
        "\n"
      ],
      "metadata": {
        "id": "W_pGyPvRrLH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## preprocess"
      ],
      "metadata": {
        "id": "T-EhuxklY2hw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "THRESHOLD = 73 \n",
        "N_FOLDS = 21\n",
        "RND_SEED = 56\n",
        "\n",
        "train_raw = pd.read_csv(\"./data/train.csv\")\n",
        "raw_ids_all = train_raw[\"Id\"]\n",
        "raw_ids = raw_ids_all.unique()\n",
        "\n",
        "####### 2. Remove ids with only NaNs in the \"Ref\" column #######\n",
        "train_raw_tmp = train_raw[~np.isnan(train_raw.Ref)]\n",
        "raw_ids_tmp = train_raw_tmp[\"Id\"].unique()\n",
        "train_new = train_raw[np.in1d(raw_ids_all, raw_ids_tmp)]\n",
        "\n",
        "####### 3. Convert all NaN to zero #######\n",
        "train_new = train_new.fillna(0.0)\n",
        "train_new = train_new.reset_index(drop=True)\n",
        "\n",
        "####### 4. Define and exclude outliers from training set #######\n",
        "train_new_group = train_new.groupby('Id')\n",
        "df = pd.DataFrame(train_new_group['Expected'].mean()) # mean, or any value\n",
        "meaningful_ids = np.array(df[df['Expected'] < THRESHOLD].index)\n",
        "\n",
        "####### 5. Split off holdout validation subset #######\n",
        "# Count the no. of observations per hour for each gauge reading\n",
        "train_new_ids_all = train_new[\"Id\"]\n",
        "obs_freq = train_new_ids_all.value_counts(ascending=True)\n",
        "obs_bins = obs_freq.unique()\n",
        "obs_num = ([(obs_freq==i).sum() for i in obs_bins])\n",
        "obs_ids = [np.array(obs_freq.index[obs_freq.values==i]) for i in obs_bins]\n",
        "\n",
        "# Construct stratified c.v. holdout set w.r.t. no. observations per hour\n",
        "y = np.array(obs_freq)\n",
        "X = np.concatenate(obs_ids)\n",
        "\n",
        "rng = np.random.RandomState(RND_SEED)\n",
        "\n",
        "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=rng)\n",
        "skf.get_n_splits(X, y)\n",
        "\n",
        "#skf = cross_validation.StratifiedKFold(y, n_folds=N_FOLDS, shuffle=True, random_state=rng) \n",
        "#skf = StratifiedKFold(n_splits=N_FOLDS, y, shuffle=True, random_state=rng)\n",
        "\n",
        "X_train_list = []\n",
        "X_valid_list = []\n",
        "\n",
        "cv = 0\n",
        "for train_index, valid_index in skf.split(X,y):\n",
        "    X_train, X_valid = X[train_index], X[valid_index]\n",
        "    print(\"train.shape before: %s\" % (X_train.shape))\n",
        "    X_train = X_train[np.in1d(X_train, meaningful_ids)]\n",
        "    \n",
        "    X_train_list.append(X_train)\n",
        "    X_valid_list.append(X_valid)\n",
        "    print(\"train.shape after: %s\" % (X_train.shape))\n",
        "    print(\"valid.shape: %s\" % (X_valid.shape))\n",
        "    \n",
        "    cv += 1\n",
        "    break # remove if full n-fold cross-validation is desired\n",
        "\n",
        "np.save(\"./data/processed_train\", np.array(train_new))\n",
        "\n",
        "####### 5. Save the partitioned IDs into folders #######\n",
        "if not os.path.exists(\"train\"):\n",
        "    os.makedirs(\"train\")\n",
        "if not os.path.exists(\"valid\"):\n",
        "    os.makedirs(\"valid\")\n",
        "if not os.path.exists(\"test\"):\n",
        "    os.makedirs(\"test\")\n",
        "    \n",
        "for i, item in enumerate(X_train_list):\n",
        "    np.save(\"./train/obs_ids_train_cv%s\" % (i), item)\n",
        "\n",
        "for i, item in enumerate(X_valid_list):\n",
        "    np.save(\"./valid/obs_ids_valid_cv%s\" % (i), item)\n",
        "\n",
        "####### 6. Preprocess the test data #######\n",
        "test_raw = pd.read_csv(\"./data/test.csv\")\n",
        "test_raw_ids_all = test_raw[\"Id\"]\n",
        "test_raw_ids = np.array(test_raw_ids_all.unique())\n",
        "\n",
        "# Convert all NaNs to zero\n",
        "test_new = test_raw.fillna(0.0)\n",
        "test_new = test_new.reset_index(drop=True)\n",
        "\n",
        "np.save(\"./data/processed_test\", np.array(test_new))\n",
        "np.save(\"./test/obs_ids_test\", test_raw_ids)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUcBQnm4rp7s",
        "outputId": "cb247b8b-bd61-4d22-c5c6-b8f5f5556556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:680: UserWarning: The least populated class in y has only 5 members, which is less than n_splits=21.\n",
            "  UserWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train.shape before: 5097\n",
            "train.shape after: 5013\n",
            "valid.shape: 255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data_augmentation_train.py\n"
      ],
      "metadata": {
        "id": "mqP_q8cWZnVM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_WIDTH = 19 # Any length >= 19, which is the max no. of time obs per hour\n",
        "CV = 0 # Cross validation set label\n",
        "NUM_RAND = 60 # No. of augmented samples\n",
        "COLUMNS = ['Id','minutes_past', 'radardist_km', 'Ref', 'Ref_5x5_10th',\n",
        "       'Ref_5x5_50th', 'Ref_5x5_90th', 'RefComposite',\n",
        "       'RefComposite_5x5_10th', 'RefComposite_5x5_50th',\n",
        "       'RefComposite_5x5_90th', 'RhoHV', 'RhoHV_5x5_10th',\n",
        "       'RhoHV_5x5_50th', 'RhoHV_5x5_90th', 'Zdr', 'Zdr_5x5_10th',\n",
        "       'Zdr_5x5_50th', 'Zdr_5x5_90th', 'Kdp', 'Kdp_5x5_10th',\n",
        "       'Kdp_5x5_50th', 'Kdp_5x5_90th', 'Expected']\n",
        "\n",
        "\n",
        "def extend_series(X, rng, target_len=19):\n",
        "    curr_len = X.shape[0]\n",
        "    extra_needed = target_len-curr_len\n",
        "    if (extra_needed > 0):\n",
        "        reps = [1]*(curr_len)\n",
        "        add_ind = rng.randint(0, curr_len, size=extra_needed)\n",
        "        \n",
        "        new_reps = [np.sum(add_ind==j) for j in range(curr_len)]\n",
        "        new_reps = np.array(reps) + np.array(new_reps)\n",
        "        X = np.repeat(X, new_reps, axis=0)\n",
        "    return X\n",
        "\n",
        "\n",
        "####### 2. Create random seeds #######\n",
        "# Any lists would do...\n",
        "rng_seed_list1 = [234561, 23451, 2341, 231, 21, 678901, 67891, 6781, 671, 16, 77177]\n",
        "rng_seed_list2 = list(range(9725, 9727+50*7, 7))\n",
        "rng_seed_list3 = list(range(9726, 9728+50*7, 7))\n",
        "#rng_seed_list = rng_seed_list1 + rng_seed_list2 + rng_seed_list3\n",
        "rng_seed_list = []\n",
        "rng_seed_list.extend(rng_seed_list1) \n",
        "rng_seed_list.extend(rng_seed_list2) \n",
        "rng_seed_list.extend(rng_seed_list3) \n",
        "\n",
        "assert len(rng_seed_list) >= NUM_RAND\n",
        "\n",
        "####### 3. Augment training data #######\n",
        "data = np.load(\"./data/processed_train.npy\")\n",
        "obs_ids_all = np.load(\"./train/obs_ids_train_cv%s.npy\" % (CV))\n",
        "\n",
        "data_pd = pd.DataFrame(data=data[:,0:], columns=COLUMNS)\n",
        "data_pd_ids_all = np.array(data_pd['Id'])\n",
        "data_pd_ids_selected = np.in1d(data_pd_ids_all, obs_ids_all)\n",
        "data_pd_filtered = data_pd[data_pd_ids_selected]\n",
        "\n",
        "data_pd_gp = pd.core.groupby.GroupBy(data_pd_filtered, \"Id\")\n",
        "data_size = len(data_pd_gp)\n",
        "\n",
        "for jj, rng_seed in enumerate(rng_seed_list[0:NUM_RAND]): \n",
        "    rng = np.random.RandomState(rng_seed) \n",
        "    output = np.empty((data_size, INPUT_WIDTH, 22))\n",
        "    y_output = np.zeros(data_size)\n",
        "    \n",
        "    i = 0\n",
        "    for _, group in data_pd_gp:\n",
        "        group_array = np.array(group)\n",
        "        X = extend_series(group_array[:,1:23], rng, target_len=INPUT_WIDTH) \n",
        "        y = group_array[0,23]\n",
        "        output[i,:,:] = X[:,:]\n",
        "        y_output[i]= y\n",
        "        i += 1\n",
        "    \n",
        "    print(\"X.shape\", X.shape)\n",
        "    print(\"output.shape\", output.shape)\n",
        "    \n",
        "    np.save(\"./train/data_train_augmented_cv%s_t%s_rand%s.npy\" %\n",
        "            (CV, INPUT_WIDTH, jj), output)\n",
        "    np.save(\"./train/data_train_expected_cv%s.npy\" % (CV), y_output) \n",
        "        \n",
        "    gc.collect()   "
      ],
      "metadata": {
        "id": "_8kxaRyAZw80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d907a0a-1827-4af2-95f8-817e5f77195f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n",
            "X.shape (19, 22)\n",
            "output.shape (5013, 19, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data_augmentation_valid.py"
      ],
      "metadata": {
        "id": "rmvEvoI-WJ9w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_WIDTH = 19 # Any length >= 19, which is the max no. of time obs per hour\n",
        "CV = 0 # Cross validation set label\n",
        "NUM_RAND = 1 # No. of augmented samples\n",
        "COLUMNS = ['Id','minutes_past', 'radardist_km', 'Ref', 'Ref_5x5_10th',\n",
        "       'Ref_5x5_50th', 'Ref_5x5_90th', 'RefComposite',\n",
        "       'RefComposite_5x5_10th', 'RefComposite_5x5_50th',\n",
        "       'RefComposite_5x5_90th', 'RhoHV', 'RhoHV_5x5_10th',\n",
        "       'RhoHV_5x5_50th', 'RhoHV_5x5_90th', 'Zdr', 'Zdr_5x5_10th',\n",
        "       'Zdr_5x5_50th', 'Zdr_5x5_90th', 'Kdp', 'Kdp_5x5_10th',\n",
        "       'Kdp_5x5_50th', 'Kdp_5x5_90th', 'Expected']\n",
        "\n",
        "\n",
        "\n",
        "def extend_series(X, rng, target_len=19):\n",
        "    curr_len = X.shape[0]\n",
        "    extra_needed = target_len-curr_len\n",
        "    if (extra_needed > 0):\n",
        "        reps = [1]*(curr_len)\n",
        "        add_ind = rng.randint(0, curr_len, size=extra_needed)\n",
        "        \n",
        "        new_reps = [np.sum(add_ind==j) for j in range(curr_len)]\n",
        "        new_reps = np.array(reps) + np.array(new_reps)\n",
        "        X = np.repeat(X, new_reps, axis=0)\n",
        "    return X\n",
        "\n",
        "\n",
        "####### 2. Create random seeds #######\n",
        "# Any lists would do...\n",
        "rng_seed_list1 = [234561, 23451, 2341, 231, 21, 678901, 67891, 6781, 671, 16, 77177]\n",
        "rng_seed_list2 = list(range(9725, 9727+50*7, 7))\n",
        "rng_seed_list3 = list(range(9726, 9728+50*7, 7))\n",
        "rng_seed_list = []\n",
        "rng_seed_list.extend(rng_seed_list1)\n",
        "rng_seed_list.extend(rng_seed_list2)\n",
        "rng_seed_list.extend(rng_seed_list3)\n",
        "assert len(rng_seed_list) >= NUM_RAND\n",
        "\n",
        "####### 3. Augment training data #######\n",
        "data = np.load(\"./data/processed_train.npy\")\n",
        "obs_ids_all = np.load(\"./valid/obs_ids_valid_cv%s.npy\" % (CV))\n",
        "\n",
        "data_pd = pd.DataFrame(data=data[:,0:], columns=COLUMNS)\n",
        "data_pd_ids_all = np.array(data_pd['Id'])\n",
        "data_pd_ids_selected = np.in1d(data_pd_ids_all, obs_ids_all)\n",
        "data_pd_filtered = data_pd[data_pd_ids_selected]\n",
        "\n",
        "data_pd_gp = pd.core.groupby.GroupBy(data_pd_filtered, \"Id\")\n",
        "data_size = len(data_pd_gp)\n",
        "\n",
        "for jj, rng_seed in enumerate(rng_seed_list[0:NUM_RAND]):\n",
        "    rng = np.random.RandomState(rng_seed) \n",
        "    output = np.empty((data_size, INPUT_WIDTH, 22))\n",
        "    y_output = np.zeros(data_size)\n",
        "    \n",
        "    i = 0\n",
        "    for _, group in data_pd_gp:\n",
        "        group_array = np.array(group)\n",
        "        X = extend_series(group_array[:,1:23], rng, target_len=INPUT_WIDTH) \n",
        "        y = group_array[0,23]\n",
        "        output[i,:,:] = X[:,:]\n",
        "        y_output[i]= y\n",
        "        i += 1\n",
        "        \n",
        "    print(\"X.shape\", X.shape)\n",
        "    print(\"output.shape\", output.shape)\n",
        "    \n",
        "    np.save(\"./valid/data_valid_augmented_cv%s_t%s_rand%s.npy\" %\n",
        "            (CV, INPUT_WIDTH, jj), output)\n",
        "    np.save(\"./valid/data_valid_expected_cv%s.npy\" % (CV), y_output) \n",
        "        \n",
        "    gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTKxaKYsWigE",
        "outputId": "f9e94b66-b43d-491d-ae07-db766cd63dce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X.shape (19, 22)\n",
            "output.shape (255, 19, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## data_augmentation_test.py"
      ],
      "metadata": {
        "id": "ND_8Hls3YSak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_WIDTH = 19 # Any length >= 19, which is the max no. of time obs per hour\n",
        "NUM_RAND = 61 # No. of augmented samples\n",
        "COLUMNS = ['Id','minutes_past', 'radardist_km', 'Ref', 'Ref_5x5_10th',\n",
        "       'Ref_5x5_50th', 'Ref_5x5_90th', 'RefComposite',\n",
        "       'RefComposite_5x5_10th', 'RefComposite_5x5_50th',\n",
        "       'RefComposite_5x5_90th', 'RhoHV', 'RhoHV_5x5_10th',\n",
        "       'RhoHV_5x5_50th', 'RhoHV_5x5_90th', 'Zdr', 'Zdr_5x5_10th',\n",
        "       'Zdr_5x5_50th', 'Zdr_5x5_90th', 'Kdp', 'Kdp_5x5_10th',\n",
        "       'Kdp_5x5_50th', 'Kdp_5x5_90th']\n",
        "       \n",
        "       \n",
        "####### 1. Define 'dropin' augmentation function #######\n",
        "def extend_series(X, rng, target_len=19):\n",
        "    curr_len = X.shape[0]\n",
        "    extra_needed = target_len-curr_len\n",
        "    if (extra_needed > 0):\n",
        "        reps = [1]*(curr_len)\n",
        "        add_ind = rng.randint(0, curr_len, size=extra_needed)\n",
        "        \n",
        "        new_reps = [np.sum(add_ind==j) for j in range(curr_len)]\n",
        "        new_reps = np.array(reps) + np.array(new_reps)\n",
        "        X = np.repeat(X, new_reps, axis=0)\n",
        "    return X\n",
        "\n",
        "####### 2. Create random seeds #######\n",
        "# Any lists would do...\n",
        "rng_seed_list1 = [234561, 23451, 2341, 231, 21, 678901, 67891, 6781, 671, 16, 77177]\n",
        "rng_seed_list2 = list(range(9725, 9727+50*7, 7))\n",
        "rng_seed_list3 = list(range(9726, 9728+50*7, 7))\n",
        "rng_seed_list = []\n",
        "rng_seed_list.extend(rng_seed_list1)\n",
        "rng_seed_list.extend(rng_seed_list2)\n",
        "rng_seed_list.extend(rng_seed_list3)\n",
        "assert len(rng_seed_list) >= NUM_RAND\n",
        "\n",
        "####### 3. Augment training data #######\n",
        "data = np.load(\"./data/processed_test.npy\")\n",
        "obs_ids_all = np.load(\"./test/obs_ids_test.npy\")\n",
        "\n",
        "data_pd = pd.DataFrame(data=data[:,0:], columns=COLUMNS)\n",
        "data_pd_ids_all = np.array(data_pd['Id'])\n",
        "data_pd_ids_selected = np.in1d(data_pd_ids_all, obs_ids_all)\n",
        "data_pd_filtered = data_pd[data_pd_ids_selected]\n",
        "\n",
        "data_pd_gp = pd.core.groupby.GroupBy(data_pd_filtered, \"Id\")\n",
        "data_size = len(data_pd_gp)\n",
        "\n",
        "for jj, rng_seed in enumerate(rng_seed_list[0:NUM_RAND]):\n",
        "    rng = np.random.RandomState(rng_seed) \n",
        "    output = np.empty((data_size, INPUT_WIDTH, 22))\n",
        "    \n",
        "    i = 0\n",
        "    for _, group in data_pd_gp:\n",
        "        group_array = np.array(group)\n",
        "        X = extend_series(group_array[:,1:23], rng, target_len=INPUT_WIDTH) \n",
        "        output[i,:,:] = X[:,:]\n",
        "        i += 1\n",
        "        \n",
        "    print(\"X.shape\", X.shape)\n",
        "    print(\"output.shape\", output.shape)\n",
        "    \n",
        "    np.save(\"./test/data_test_augmented_t%s_rand%s.npy\" % (INPUT_WIDTH, jj), output)\n",
        "        \n",
        "    gc.collect()\n",
        "    \n",
        "    \n"
      ],
      "metadata": {
        "id": "eK5Rk7NoYXZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NNregression_v1.py -v=1"
      ],
      "metadata": {
        "id": "i_n-mRVnY7v4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1flTSYdB79ZQ",
        "outputId": "ab4ea181-c2a1-4269-cf90-e36b6124a22f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def do_regression(num_epochs=60, # No. of epochs to train\n",
        "                  init_file=None,  # Saved parameters to initialise training\n",
        "                  epoch_size=680780,  # Whole dataset size\n",
        "                  valid_size=34848,\n",
        "                  train_batch_multiple=10637,  # No. of minibatches per batch\n",
        "                  valid_batch_multiple=1089,  # No. of minibatches per batch\n",
        "                  train_minibatch_size=64, \n",
        "                  valid_minibatch_size=32,\n",
        "                  eval_multiple=50,  # No. of minibatches to ave. in report\n",
        "                  save_model=True,\n",
        "                  input_width=19,\n",
        "                  rng_seed=100009,\n",
        "                  cross_val=0,  # Cross-validation subset label\n",
        "                  dataver=1,  # Label for different runs/architectures/etc\n",
        "                  rate_init=1.0,\n",
        "                  rate_decay=0.999983):\n",
        "\n",
        "    ###################################################\n",
        "    ################# 0. User inputs ##################\n",
        "    ###################################################\n",
        "    for i in range(1,len(sys.argv)):\n",
        "        if sys.argv[i].startswith('-'):\n",
        "            option = sys.argv[i][1:]\n",
        "            if option == 'i': init_file = sys.argv[i+1]\n",
        "            elif option[0:2] == 'v=' : dataver = int(option[2:])\n",
        "            elif option[0:3] == 'cv=' : cross_val = int(option[3:])\n",
        "            elif option[0:3] == 'rs=' : rng_seed = int(option[3:])\n",
        "            elif option[0:3] == 'ri=' : rate_init = np.float32(option[3:])\n",
        "            elif option[0:3] == 'rd=' : rate_decay = np.float32(option[3:])\n",
        "                                \n",
        "    print(\"Running with dataver %s\" % (dataver))\n",
        "    print(\"Running with cross_val %s\" % (cross_val))\n",
        "    \n",
        "    \n",
        "    ###################################################\n",
        "    ############# 1. Housekeeping values ##############\n",
        "    ###################################################\n",
        "    # Batch size is possibly not equal to epoch size due to memory limits\n",
        "    train_batch_size = train_batch_multiple*train_minibatch_size \n",
        "    assert epoch_size >= train_batch_size\n",
        "    \n",
        "    # Number of times we expect the training/validation generator to be called\n",
        "    max_train_gen_calls = (num_epochs*epoch_size)//train_batch_size \n",
        "\n",
        "    # Number of evaluations (total minibatches / eval_multiple)\n",
        "    num_eval = max_train_gen_calls*train_batch_multiple / eval_multiple\n",
        "    \n",
        "    \n",
        "    ###################################################\n",
        "    ###### 2. Define model and theano variables #######\n",
        "    ###################################################\n",
        "    if rng_seed is not None:\n",
        "        print(\"Setting RandomState with seed=%i\" % (rng_seed))\n",
        "        rng = np.random.RandomState(rng_seed)\n",
        "        set_rng(rng)\n",
        "    \n",
        "    print(\"Defining variables...\")\n",
        "    index = T.lscalar() # Minibatch index\n",
        "    x = T.tensor3,'x') # Inputs \n",
        "    y = T.fvector('y') # Target\n",
        "\n",
        "    \n",
        "    print(\"Defining model...\")\n",
        "    network_0 = build_1Dregression_v1(\n",
        "                        input_var=x,\n",
        "                        input_width=input_width,\n",
        "                        nin_units=12,\n",
        "                        h_num_units=[64,128,256,128,64],\n",
        "                        h_grad_clip=1.0,\n",
        "                        output_width=1\n",
        "                        )\n",
        "                        \n",
        "    if init_file is not None:\n",
        "        print(\"Loading initial model parametrs...\")\n",
        "        init_model = np.load(init_file)\n",
        "        init_params = init_model[init_model.files[0]]           \n",
        "        LL.set_all_param_values([network_0], init_params)\n",
        "        \n",
        "    \n",
        "    ###################################################                                \n",
        "    ################ 3. Import data ###################\n",
        "    ###################################################\n",
        "    # Loading data generation model parameters\n",
        "    print(\"Defining shared variables...\")\n",
        "    train_set_y = theano.shared(np.zeros(1, dtype=theano.config.floatX),\n",
        "                                borrow=True) \n",
        "    train_set_x = theano.shared(np.zeros((1,1,1), dtype=theano.config.floatX),\n",
        "                                borrow=True)\n",
        "    \n",
        "    valid_set_y = theano.shared(np.zeros(1, dtype=theano.config.floatX),\n",
        "                                borrow=True)\n",
        "    valid_set_x = theano.shared(np.zeros((1,1,1), dtype=theano.config.floatX),\n",
        "                                borrow=True)\n",
        "    \n",
        "    # Validation data (pick a single augmented instance, rand0 here)\n",
        "    print(\"Creating validation data...\")    \n",
        "    chunk_valid_data = np.load(\n",
        "        \"./valid/data_valid_augmented_cv%s_t%s_rand0.npy\" \n",
        "        % (cross_val, input_width)\n",
        "        ).astype(theano.config.floatX)\n",
        "    chunk_valid_answers = np.load(\n",
        "        \"./valid/data_valid_expected_cv%s.npy\" \n",
        "        % (cross_val)\n",
        "        ).astype(theano.config.floatX)     \n",
        "    \n",
        "    print(\"chunk_valid_answers.shape\", chunk_valid_answers.shape)\n",
        "    print(\"Assigning validation data...\")\n",
        "    valid_set_y.set_value(chunk_valid_answers[:])\n",
        "    valid_set_x.set_value(chunk_valid_data.transpose(0,2,1))\n",
        "    \n",
        "    # Create output directory\n",
        "    if not os.path.exists(\"output_cv%s_v%s\" % (cross_val, dataver)):\n",
        "        os.makedirs(\"output_cv%s_v%s\" % (cross_val, dataver))\n",
        "    \n",
        "    \n",
        "    ###################################################                                \n",
        "    ########### 4. Create Loss expressions ############\n",
        "    ###################################################\n",
        "    print(\"Defining loss expressions...\")\n",
        "    prediction_0 = LL.get_output(network_0) \n",
        "    train_loss = aggregate(T.abs_(prediction_0 - y.dimshuffle(0,'x')))\n",
        "    print(type(train_loss))\n",
        "    valid_prediction_0 = LL.get_output(network_0, deterministic=True)\n",
        "    valid_loss = aggregate(T.abs_(valid_prediction_0 - y.dimshuffle(0,'x')))\n",
        "    \n",
        "    \n",
        "    ###################################################                                \n",
        "    ############ 5. Define update method  #############\n",
        "    ###################################################\n",
        "    print(\"Defining update choices...\")\n",
        "    params = LL.get_all_params(network_0, trainable=True)\n",
        "    learn_rate = T.scalar('learn_rate', dtype=theano.config.floatX)\n",
        "    \n",
        "    updates = lasagne.updates.adadelta(train_loss, params,\n",
        "                                       learning_rate=learn_rate)\n",
        "    \n",
        "    \n",
        "    ###################################################                                \n",
        "    ######### 6. Define train/valid functions #########\n",
        "    ###################################################    \n",
        "    print(\"Defining theano functions...\")\n",
        "    #y = T.cast(y, 'float64')\n",
        "    train_set_y = train_set_y.reshape(train_set_y.shape[0], -1)\n",
        "    \n",
        "    train_model = theano.function(\n",
        "        [index, learn_rate],\n",
        "        train_loss,\n",
        "        updates=updates,\n",
        "        #on_unused_input='ignore',\n",
        "        givens={\n",
        "            x: train_set_x[(index*train_minibatch_size): ((index+1)*train_minibatch_size)],\n",
        "            y: train_set_y[(index*train_minibatch_size): ((index+1)*train_minibatch_size)]  \n",
        "        }\n",
        "    )\n",
        "    \n",
        "    validate_model = theano.function(\n",
        "        [index],\n",
        "        valid_loss,\n",
        "        givens={\n",
        "            x: valid_set_x[index*valid_minibatch_size: (index+1)*valid_minibatch_size],\n",
        "            y: valid_set_y[index*valid_minibatch_size: (index+1)*valid_minibatch_size]\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    \n",
        "    ###################################################                                \n",
        "    ################ 7. Begin training ################\n",
        "    ###################################################  \n",
        "    print(\"Begin training...\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    cum_iterations = 0\n",
        "    this_train_loss = 0.0\n",
        "    this_valid_loss = 0.0\n",
        "    best_valid_loss = np.inf\n",
        "    best_iter = 0\n",
        "    \n",
        "    train_eval_scores = np.empty(int(num_eval))\n",
        "    valid_eval_scores = np.empty(int(num_eval))\n",
        "    eval_index = 0\n",
        "    aug_index = 0\n",
        "    \n",
        "    for batch in range(max_train_gen_calls):\n",
        "        start_time = time.time()        \n",
        "        chunk_train_data = np.load(\n",
        "            \"./train/data_train_augmented_cv%s_t%s_rand%s.npy\" %\n",
        "            (cross_val, input_width, aug_index)\n",
        "            ).astype(theano.config.floatX)\n",
        "        chunk_train_answers = np.load(\n",
        "            \"./train/data_train_expected_cv%s.npy\" % \n",
        "            (cross_val)\n",
        "            ).astype(theano.config.floatX)     \n",
        "            \n",
        "        train_set_y.set_value(chunk_train_answers[:])\n",
        "        train_set_x.set_value(chunk_train_data.transpose(0, 2, 1))\n",
        "        \n",
        "        # Iterate over minibatches in each batch\n",
        "        for mini_index in range(train_batch_multiple):\n",
        "            this_rate = np.float32(rate_init*(rate_decay**cum_iterations))\n",
        "            this_train_loss += train_model(mini_index, this_rate)\n",
        "            cum_iterations += 1\n",
        "            \n",
        "            # Report loss \n",
        "            if (cum_iterations % eval_multiple == 0):\n",
        "                this_train_loss = this_train_loss / eval_multiple\n",
        "                this_valid_loss = np.mean([validate_model(i) for\n",
        "                                    i in range(valid_batch_multiple)])\n",
        "                train_eval_scores[eval_index] = this_train_loss\n",
        "                valid_eval_scores[eval_index] = this_valid_loss\n",
        "                \n",
        "                # Save report every five evaluations\n",
        "                if ((eval_index+1) % 5 == 0):\n",
        "                    np.savetxt(\n",
        "                        \"output_cv%s_v%s/training_scores.txt\" %\n",
        "                        (cross_val, dataver),\n",
        "                         train_eval_scores, fmt=\"%.5f\"\n",
        "                         )\n",
        "                    np.savetxt(\n",
        "                        \"output_cv%s_v%s/validation_scores.txt\" %\n",
        "                        (cross_val, dataver),\n",
        "                         valid_eval_scores, fmt=\"%.5f\"\n",
        "                         )\n",
        "                    np.savetxt(\n",
        "                        \"output_cv%s_v%s/last_learn_rate.txt\" %\n",
        "                        (cross_val, dataver),\n",
        "                        [np.array(this_rate)], fmt=\"%.5f\"\n",
        "                        )\n",
        "                \n",
        "                # Save model if best validation score\n",
        "                if (this_valid_loss < best_valid_loss):  \n",
        "                    best_valid_loss = this_valid_loss\n",
        "                    best_iter = cum_iterations-1\n",
        "                    \n",
        "                    if save_model:\n",
        "                        np.savez(\"output_cv%s_v%s/model.npz\" % \n",
        "                                 (cross_val, dataver),\n",
        "                                 LL.get_all_param_values(network_0))\n",
        "                    \n",
        "                # Reset evaluation reports\n",
        "                eval_index += 1\n",
        "                this_train_loss = 0.0\n",
        "                this_valid_loss = 0.0\n",
        "                \n",
        "        aug_index += 1\n",
        "        \n",
        "        end_time = time.time()\n",
        "        print(\"Computing time for batch %d: %f\" % (batch, end_time-start_time))\n",
        "        \n",
        "    print(\"Best validation loss %f after %d epochs\" %\n",
        "          (best_valid_loss, (best_iter*train_minibatch_size//epoch_size)))\n",
        "    \n",
        "    del train_set_x, train_set_y, valid_set_x, valid_set_y\n",
        "    gc.collect()\n",
        "    \n",
        "    return None\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    sys.argv.append(\"-v=1\")\n",
        "    print(sys.argv)\n",
        "    do_regression()     \n"
      ],
      "metadata": {
        "id": "kQljPgaMY864",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "outputId": "2b4e887e-ad66-4d8a-fac3-d8f70de14dda"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py', '-f', '/root/.local/share/jupyter/runtime/kernel-7fd6c6b8-0091-4c7c-bd5c-8828cc916acd.json', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1', '-v=1']\n",
            "Running with dataver 1\n",
            "Running with cross_val 0\n",
            "Setting RandomState with seed=100009\n",
            "Defining variables...\n",
            "Defining model...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-ecd36b17a8e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-v=1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0mdo_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-41-ecd36b17a8e7>\u001b[0m in \u001b[0;36mdo_regression\u001b[0;34m(num_epochs, init_file, epoch_size, valid_size, train_batch_multiple, valid_batch_multiple, train_minibatch_size, valid_minibatch_size, eval_multiple, save_model, input_width, rng_seed, cross_val, dataver, rate_init, rate_decay)\u001b[0m\n\u001b[1;32m     68\u001b[0m                         \u001b[0mh_num_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                         \u001b[0mh_grad_clip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                         \u001b[0moutput_width\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                         )\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/NN_architectures.py\u001b[0m in \u001b[0;36mbuild_1Dregression_v1\u001b[0;34m(input_var, input_width, nin_units, h_num_units, h_grad_clip, output_width)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;31m# Input layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     l_in = LL.InputLayer(shape=(None, 22, input_width), \n\u001b[0;32m---> 39\u001b[0;31m                             input_var=input_var) \n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mbatchsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ml_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/lasagne/layers/input.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, shape, input_var, name, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_var\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 raise ValueError(\"shape has %d dimensions, but variable has \"\n\u001b[0;32m---> 69\u001b[0;31m                                  \"%d\" % (ndim, input_var.ndim))\n\u001b[0m\u001b[1;32m     70\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: shape has 3 dimensions, but variable has 4"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NNregression_v2.py -v=2"
      ],
      "metadata": {
        "id": "w4SEAKU5BJ3o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "############################### Main ################################\n",
        "def do_regression(num_epochs=60, # No. of epochs to train\n",
        "                  init_file=None,  # Saved parameters to initialise training\n",
        "                  epoch_size=680780,  # Whole dataset size\n",
        "                  valid_size=34848, # Size of validation holdout set\n",
        "                  train_batch_multiple=10637,  # No. of minibatches per batch\n",
        "                  valid_batch_multiple=1089,  # No. of minibatches per batch\n",
        "                  train_minibatch_size=64,\n",
        "                  valid_minibatch_size=32,\n",
        "                  eval_multiple=50,  # No. of minibatches to ave. in report\n",
        "                  save_model=True,\n",
        "                  input_width=19,\n",
        "                  rng_seed=100005,\n",
        "                  cross_val=0,  # Cross-validation subset label\n",
        "                  dataver=2,  # Label for different runs/architectures/etc\n",
        "                  rate_init=1.0,\n",
        "                  rate_decay=0.999983):\n",
        "\n",
        "    ###################################################\n",
        "    ################# 0. User inputs ##################\n",
        "    ###################################################\n",
        "    for i in range(1,len(sys.argv)):\n",
        "        if sys.argv[i].startswith('-'):\n",
        "            option = sys.argv[i][1:]\n",
        "            if option == 'i': init_file = sys.argv[i+1]\n",
        "            elif option[0:2] == 'v=' : dataver = int(option[2:])\n",
        "            elif option[0:3] == 'cv=' : cross_val = int(option[3:])\n",
        "            elif option[0:3] == 'rs=' : rng_seed = int(option[3:])\n",
        "            elif option[0:3] == 'ri=' : rate_init = np.float32(option[3:])\n",
        "            elif option[0:3] == 'rd=' : rate_decay = np.float32(option[3:])\n",
        "                                \n",
        "    print(\"Running with dataver %s\" % (dataver))\n",
        "    print(\"Running with cross_val %s\" % (cross_val))\n",
        "    \n",
        "    \n",
        "    ###################################################\n",
        "    ############# 1. Housekeeping values ##############\n",
        "    ###################################################\n",
        "    # Batch size is possibly not equal to epoch size due to memory limits\n",
        "    train_batch_size = train_batch_multiple*train_minibatch_size \n",
        "    assert epoch_size >= train_batch_size\n",
        "    \n",
        "    # Number of times we expect the training/validation generator to be called\n",
        "    max_train_gen_calls = (num_epochs*epoch_size)//train_batch_size \n",
        "\n",
        "    # Number of evaluations (total minibatches / eval_multiple)\n",
        "    num_eval = max_train_gen_calls*train_batch_multiple / eval_multiple\n",
        "    \n",
        "    \n",
        "    ###################################################\n",
        "    ###### 2. Define model and theano variables #######\n",
        "    ###################################################\n",
        "    if rng_seed is not None:\n",
        "        print(\"Setting RandomState with seed=%i\" % (rng_seed))\n",
        "        rng = np.random.RandomState(rng_seed)\n",
        "        set_rng(rng)\n",
        "    \n",
        "    print(\"Defining variables...\")\n",
        "    index = T.lscalar() # Minibatch index\n",
        "    x = T.tensor3('x') # Inputs \n",
        "    y = T.fvector('y') # Target\n",
        "    \n",
        "    print(\"Defining model...\")\n",
        "    network_0 = build_1Dregression_v2(\n",
        "                        input_var=x,\n",
        "                        input_width=input_width,\n",
        "                        h_num_units=[120,120,120],\n",
        "                        h_grad_clip=1.0,\n",
        "                        output_width=1\n",
        "                        )\n",
        "                        \n",
        "    if init_file is not None:\n",
        "        print(\"Loading initial model parametrs...\")\n",
        "        init_model = np.load(init_file)\n",
        "        init_params = init_model[init_model.files[0]]           \n",
        "        LL.set_all_param_values([network_0], init_params)\n",
        "        \n",
        "    \n",
        "    ###################################################                                \n",
        "    ################ 3. Import data ###################\n",
        "    ###################################################\n",
        "    # Loading data generation model parameters\n",
        "    print(\"Defining shared variables...\")\n",
        "    train_set_y = theano.shared(np.zeros(1, dtype=theano.config.floatX),\n",
        "                                borrow=True) \n",
        "    train_set_x = theano.shared(np.zeros((1,1,1), dtype=theano.config.floatX),\n",
        "                                borrow=True)\n",
        "    \n",
        "    valid_set_y = theano.shared(np.zeros(1, dtype=theano.config.floatX),\n",
        "                                borrow=True)\n",
        "    valid_set_x = theano.shared(np.zeros((1,1,1), dtype=theano.config.floatX),\n",
        "                                borrow=True)\n",
        "    \n",
        "    # Validation data (pick a single augmented instance, rand0 here)\n",
        "    print(\"Creating validation data...\")    \n",
        "    chunk_valid_data = np.load(\n",
        "        \"./valid/data_valid_augmented_cv%s_t%s_rand0.npy\" \n",
        "        % (cross_val, input_width)\n",
        "        ).astype(theano.config.floatX)\n",
        "    chunk_valid_answers = np.load(\n",
        "        \"./valid/data_valid_expected_cv%s.npy\" \n",
        "        % (cross_val)\n",
        "        ).astype(theano.config.floatX)     \n",
        "    \n",
        "    print(\"chunk_valid_answers.shape\", chunk_valid_answers.shape)\n",
        "    print(\"Assigning validation data...\")\n",
        "    valid_set_y.set_value(chunk_valid_answers[:])\n",
        "    valid_set_x.set_value(chunk_valid_data.transpose(0,2,1))\n",
        "    \n",
        "    # Create output directory\n",
        "    if not os.path.exists(\"output_cv%s_v%s\" % (cross_val, dataver)):\n",
        "        os.makedirs(\"output_cv%s_v%s\" % (cross_val, dataver))\n",
        "    \n",
        "    \n",
        "    ###################################################                                \n",
        "    ########### 4. Create Loss expressions ############\n",
        "    ###################################################\n",
        "    print(\"Defining loss expressions...\")\n",
        "    prediction_0 = LL.get_output(network_0) \n",
        "    train_loss = aggregate(T.abs_(prediction_0 - y.dimshuffle(0,'x')))\n",
        "    \n",
        "    valid_prediction_0 = LL.get_output(network_0, deterministic=True)\n",
        "    valid_loss = aggregate(T.abs_(valid_prediction_0 - y.dimshuffle(0,'x')))\n",
        "    \n",
        "    \n",
        "    ###################################################                                \n",
        "    ############ 5. Define update method  #############\n",
        "    ###################################################\n",
        "    print(\"Defining update choices...\")\n",
        "    params = LL.get_all_params(network_0, trainable=True)\n",
        "    learn_rate = T.scalar('learn_rate', dtype=theano.config.floatX)\n",
        "    \n",
        "    updates = lasagne.updates.adadelta(train_loss, params,\n",
        "                                       learning_rate=learn_rate)\n",
        "    \n",
        "    \n",
        "    ###################################################                                \n",
        "    ######### 6. Define train/valid functions #########\n",
        "    ###################################################    \n",
        "    print(\"Defining theano functions...\")\n",
        "    train_model = theano.function(\n",
        "        [index, learn_rate],\n",
        "        train_loss,\n",
        "        updates=updates,\n",
        "        givens={\n",
        "            x: train_set_x[(index*train_minibatch_size):\n",
        "                            ((index+1)*train_minibatch_size)],\n",
        "            y: train_set_y[(index*train_minibatch_size):\n",
        "                            ((index+1)*train_minibatch_size)]  \n",
        "        }\n",
        "    )\n",
        "    \n",
        "    validate_model = theano.function(\n",
        "        [index],\n",
        "        valid_loss,\n",
        "        givens={\n",
        "            x: valid_set_x[index*valid_minibatch_size:\n",
        "                            (index+1)*valid_minibatch_size],\n",
        "            y: valid_set_y[index*valid_minibatch_size:\n",
        "                            (index+1)*valid_minibatch_size]\n",
        "        }\n",
        "    )\n",
        "    \n",
        "    \n",
        "    ###################################################                                \n",
        "    ################ 7. Begin training ################\n",
        "    ###################################################  \n",
        "    print(\"Begin training...\")\n",
        "    sys.stdout.flush()\n",
        "    \n",
        "    cum_iterations = 0\n",
        "    this_train_loss = 0.0\n",
        "    this_valid_loss = 0.0\n",
        "    best_valid_loss = np.inf\n",
        "    best_iter = 0\n",
        "    \n",
        "    train_eval_scores = np.empty(num_eval)\n",
        "    valid_eval_scores = np.empty(num_eval)\n",
        "    eval_index = 0\n",
        "    aug_index = 0\n",
        "    \n",
        "    for batch in range(max_train_gen_calls):\n",
        "        start_time = time.time()        \n",
        "        chunk_train_data = np.load(\n",
        "            \"./train/data_train_augmented_cv%s_t%s_rand%s.npy\" %\n",
        "            (cross_val, input_width, aug_index)\n",
        "            ).astype(theano.config.floatX)\n",
        "        chunk_train_answers = np.load(\n",
        "            \"./train/data_train_expected_cv%s.npy\" % \n",
        "            (cross_val)\n",
        "            ).astype(theano.config.floatX)     \n",
        "            \n",
        "        train_set_y.set_value(chunk_train_answers[:])\n",
        "        train_set_x.set_value(chunk_train_data.transpose(0, 2, 1))\n",
        "        \n",
        "        # Iterate over minibatches in each batch\n",
        "        for mini_index in range(train_batch_multiple):\n",
        "            this_rate = np.float32(rate_init*(rate_decay**cum_iterations))\n",
        "            this_train_loss += train_model(mini_index, this_rate)\n",
        "            cum_iterations += 1\n",
        "            \n",
        "            # Report loss \n",
        "            if (cum_iterations % eval_multiple == 0):\n",
        "                this_train_loss = this_train_loss / eval_multiple\n",
        "                this_valid_loss = np.mean([validate_model(i) for\n",
        "                                    i in range(valid_batch_multiple)])\n",
        "                train_eval_scores[eval_index] = this_train_loss\n",
        "                valid_eval_scores[eval_index] = this_valid_loss\n",
        "                \n",
        "                # Save report every five evaluations\n",
        "                if ((eval_index+1) % 5 == 0):\n",
        "                    np.savetxt(\n",
        "                        \"output_cv%s_v%s/training_scores.txt\" %\n",
        "                        (cross_val, dataver),\n",
        "                         train_eval_scores, fmt=\"%.5f\"\n",
        "                         )\n",
        "                    np.savetxt(\n",
        "                        \"output_cv%s_v%s/validation_scores.txt\" %\n",
        "                        (cross_val, dataver),\n",
        "                         valid_eval_scores, fmt=\"%.5f\"\n",
        "                         )\n",
        "                    np.savetxt(\n",
        "                        \"output_cv%s_v%s/last_learn_rate.txt\" %\n",
        "                        (cross_val, dataver),\n",
        "                        [np.array(this_rate)], fmt=\"%.5f\"\n",
        "                        )\n",
        "                \n",
        "                # Save model if best validation score\n",
        "                if (this_valid_loss < best_valid_loss):  \n",
        "                    best_valid_loss = this_valid_loss\n",
        "                    best_iter = cum_iterations-1\n",
        "                    \n",
        "                    if save_model:\n",
        "                        np.savez(\"output_cv%s_v%s/model.npz\" % \n",
        "                                 (cross_val, dataver),\n",
        "                                 LL.get_all_param_values(network_0))\n",
        "                    \n",
        "                # Reset evaluation reports\n",
        "                eval_index += 1\n",
        "                this_train_loss = 0.0\n",
        "                this_valid_loss = 0.0\n",
        "                \n",
        "        aug_index += 1\n",
        "            \n",
        "        end_time = time.time()\n",
        "        print(\"Computing time for batch %d: %f\" % (batch, end_time-start_time))\n",
        "        \n",
        "    print(\"Best validation loss %f after %d epochs\" %\n",
        "          (best_valid_loss, (best_iter*train_minibatch_size//epoch_size)))\n",
        "    \n",
        "    del train_set_x, train_set_y, valid_set_x, valid_set_y\n",
        "    gc.collect()\n",
        "    \n",
        "    return None\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    do_regression()     "
      ],
      "metadata": {
        "id": "FF7JbfnRfQdc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "outputId": "aadcd1aa-856a-48a0-c4b3-d24f64df256f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with dataver 1\n",
            "Running with cross_val 0\n",
            "Setting RandomState with seed=100005\n",
            "Defining variables...\n",
            "Defining model...\n",
            "Defining shared variables...\n",
            "Creating validation data...\n",
            "chunk_valid_answers.shape (255,)\n",
            "Assigning validation data...\n",
            "Defining loss expressions...\n",
            "Defining update choices...\n",
            "Defining theano functions...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-ac15df88d6b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0mdo_regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-14-ac15df88d6b2>\u001b[0m in \u001b[0;36mdo_regression\u001b[0;34m(num_epochs, init_file, epoch_size, valid_size, train_batch_multiple, valid_batch_multiple, train_minibatch_size, valid_minibatch_size, eval_multiple, save_model, input_width, rng_seed, cross_val, dataver, rate_init, rate_decay)\u001b[0m\n\u001b[1;32m    149\u001b[0m                             ((index+1)*train_minibatch_size)],\n\u001b[1;32m    150\u001b[0m             y: train_set_y[(index*train_minibatch_size):\n\u001b[0;32m--> 151\u001b[0;31m                             ((index+1)*train_minibatch_size)]  \n\u001b[0m\u001b[1;32m    152\u001b[0m         }\n\u001b[1;32m    153\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/theano/compile/function/__init__.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m         )\n\u001b[1;32m    352\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/theano/compile/function/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;31m# transform params into theano.compile.In objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     inputs = [\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0m_pfunc_param_to_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_downcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     ]\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/theano/compile/function/pfunc.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;31m# transform params into theano.compile.In objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     inputs = [\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0m_pfunc_param_to_in\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_downcast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_input_downcast\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m     ]\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/theano/compile/function/pfunc.py\u001b[0m in \u001b[0;36m_pfunc_param_to_in\u001b[0;34m(param, strict, allow_downcast)\u001b[0m\n\u001b[1;32m    541\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown parameter type: {type(param)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Unknown parameter type: <class 'theano.tensor.var.TensorVariable'>"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "T-EhuxklY2hw",
        "mqP_q8cWZnVM",
        "rmvEvoI-WJ9w"
      ],
      "name": "Report_KHDLUD_NHOM1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}