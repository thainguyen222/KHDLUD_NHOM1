{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thainguyen222/KHDLUD_NHOM1/blob/main/Report_KHDLUD_NHOM1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pk6wgYYFG1bS"
      },
      "source": [
        "# Báo cáo đồ án môn \"Khoa học dữ liệu ứng dụng\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klnVHLYiLZMz"
      },
      "source": [
        "Nhóm 1:\n",
        "1. 1612571 - Trần Tiến Sỹ - AllenAlexander98\n",
        "2. 1612607 - Nguyễn Quang Thái - thainguyen222\n",
        "3. 18120228\t- Huỳnh Nhựt Quang - huynhnhutquang7\n",
        "\n",
        "Link thùng chứa Github của nhóm: https://github.com/thainguyen222/KHDLUD_NHOM1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FFBFXoMKKnww"
      },
      "source": [
        "## Mô tả bài toán"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6Owv-Z3KuTu"
      },
      "source": [
        "**Giới Thiệu Bài Toán**\n",
        "\n",
        "  Bài toán dự đoán về khả năng xuất hiện và vị trí của chủng Virus West Nile của vật truyền trung gian là muỗi, dựa trên các số liệu thu thập được ở Chicago, USA.\n",
        "\n",
        "https://www.kaggle.com/competitions/predict-west-nile-virus/\n",
        "\n",
        "  Virus West Nile là một chủng virus nguy hiểm, có thể gây sốt, ảnh hưởng đến thần kinh có thể dẫn đến tử vong. Lần đầu một ca bệnh được báo cáo ở Chicago năm 2002. Đến năm 2004, Chicago thành lập the Chicago Department of Public Health (CDPH) nhằm mục đích giám sát và kiểm soát dịch bệnh này. Hằng tuần, các bẫy bắt muỗi sẽ được xét nghiệm kiểm tra, nơi nào có nhiều muỗi có bệnh sẽ được phun xịt thuốc. Cuộc thi này được tài trợ bởi the Robert Wood Johnson Foundation. Dữ liệu được cung cấp bởi the Chicago Department of Public Health. Từ các dữ liệu thời tiết, testing, vị trí và dữ liệu về phun xịt thuốc, dự đoán thời gian nào và ở đâu sẽ có muỗi chứa mầm bệnh.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mô tả dữ liệu "
      ],
      "metadata": {
        "id": "qCzOdHnbFV3I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHZx7te1vBaw"
      },
      "source": [
        "\n",
        "Trong cuộc thi này, nhóm sẽ phân tích dữ liệu thời tiết và dữ liệu phun thuốc và dự đoán có hay không vi rút Virus West Nile trong một thời điểm, vị trí và giống loài muỗi nhất định.\n",
        "\n",
        "Hàng năm, từ cuối tháng 5 đến đầu tháng 10, các nhân viên y tế cộng đồng ở Chicago sẽ đặt các bẫy muỗi rải rác khắp nơi trong thành phố. Hàng tuần, từ thứ Hai đến thứ Tư, những chiếc bẫy này thu thập muỗi và những con muỗi này sẽ được kiểm tra xem có mắc Virus West Nile không. Các kết quả kiểm tra bao gồm số lượng muỗi, loài muỗi, và có hay không những con muôĩ mắc virus West Nile.\n",
        "\n",
        "**Main dataset**\n",
        "\n",
        "Các dòng trong dữ liệu gọi là các record. Các record đại diện cho một lần kiểm tra đối với một giống loài nhất định. Nếu số lượng muỗi trong record vượt quá 50 thì sẽ tách ra thành một record riêng biệt.\n",
        "\n",
        "Vị trí của các bẫy được mô tả bằng số block và tên đường. Để thuận tiện, địa chỉ cũng đã được ánh xạ thành kinh độ và vĩ độ trong tập dữ liệu. \n",
        "\n",
        "Một số bẫy gọi là bẫy vệ tinh được đặt xung quanh một cái bẫy khác để nâng cao khả năng giám sát (có hậu tố trong id là một chữ cái). \n",
        "\n",
        "**Spray dataset**\n",
        "\n",
        "Thành phố Chicago cũng phun thuốc diệt muỗi. Chúng ta có dữ liệu những lần phun thuốc của họ trong năm 2011 và 2013. Việc phun thuốc có thể làm giảm số lượng muỗi trong khu vực và do đó có thể loại bỏ sự xuất hiện của virus West Nile.\n",
        "\n",
        "**Weather Data**\n",
        "\n",
        "Người ta tin rằng điều kiện nóng và khô thuận lợi hơn cho virus West Nile phát triển hơn là lạnh và ẩm ướt. Chúng ta có tập dữ liệu từ NOAA về các điều kiện thời tiết của năm 2007 đến năm 2014, trong các tháng thử nghiệm. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mô tả file "
      ],
      "metadata": {
        "id": "lL82WtZ0Fc2A"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4MT9TR8O7nAD"
      },
      "source": [
        "**train.csv, test.csv** \n",
        "\n",
        "Tập trainning bao gồm dữ liệu các năm 2007, 2009, 2011 và 2013, trong khi đó tập test yêu cầu dự đoán kết quả cho các năm 2008, 2010, 2012, and 2014.\n",
        "\n",
        "- Id: id của record\n",
        "- Date: ngày kiểm tra virus \n",
        "- Address: Địa chỉ gần nơi đặt bẫy\n",
        "- Species: giống loài muỗi\n",
        "- Block: số block của địa chỉ\n",
        "- Street: tên đường\n",
        "- Trap: Id của bẫy\n",
        "- AddressNumberAndStreet: số nhà và tên đường\n",
        "- Latitude, Longitude: vĩ độ, kinh độ đặt bẫy\n",
        "- AddressAccuracy: độ chính xác của địa chỉ \n",
        "- NumMosquitos: số lượng muỗi trong một record \n",
        "- WnvPresent: có hay không muỗi nhiễm virus trong record này. 1 là có, và 0 là không.\n",
        "\n",
        "**spray.csv**\n",
        "\n",
        "Dữ liệu phun thuốc trong năm 2011 và 2013\n",
        "- Date, Time: ngày và giờ phun thuốc\n",
        "- Latitude, Longitude: vĩ độ và kinh độ địa điểm phun thuốc\n",
        "\n",
        "**weather.csv**\n",
        "\n",
        "Dữ liệu thời tiết từ năm 2007 tới 2014. Các cột được mô tả trong [link](https://github.com/diefimov/west_nile_virus_2015/blob/master/data/input/noaa_weather_qclcd_documentation.pdf)\n",
        "\n",
        "**sampleSubmission.csv**\n",
        "\n",
        "File mẫu để nộp lên Kaggle\n",
        "- Id: id của record\n",
        "- WnvPresent: có hay không muỗi nhiễm virus trong record này \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6v0ngGJLdES"
      },
      "source": [
        "## Giải quyết bài toán"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvXebiKSLyek"
      },
      "source": [
        "Trong đồ án này, nhóm tìm hiểu và cài đặt lại một phương pháp giải quyết đã có (phương pháp đứng thứ hai trong leaderboard và [link](https://github.com/diefimov/west_nile_virus_2015)) của 2 tác giả Dimitry Efimov và Leustagos. \n",
        "\n",
        "Qua quá trình tìm hiểu đồ án, nhóm em quyết định giải quyết bài toán với 2 thuật toán dựa trên Decision Tree:\n",
        "1.\tGradient Boosting Classification Decision trees\n",
        "2.\tRegularized Greedy Forest\n",
        "\n",
        "Điều đáng chú ý là các phương pháp khác chẳng hạn như Random Forest Classifier, Generalized Linear Methods, ExtraTrees cho thấy kết quả tốt nhưng không cải thiện được phần phối hợp cuối cùng. Ngoài ra, mô hình Single Regularized Greedy Forest cho độ chính xác rất tốt, đủ để giữ vị trí thứ hai trên bảng xếp hạng.\n",
        "\n",
        "Phương pháp giải quyết áp dụng cách làm mịn cho các dự đoán dựa trên những bẫy gần nhất và những ngày gần nhất. Khoảng cách Haversine đã được tính toán để tìm ra những cái bẫy gần nhất.\n",
        "\n",
        "Đối với các dự đoán của mô hình Gradient Boosting Classification từ 4 bẫy gần nhất, 3 ngày trước đó và 3 ngày trong tương lai đã được thêm vào tổ hợp tuyến tính có trọng số (bẫy gần hơn, ngày gần hơn, dự đoán có trọng số cao hơn). \n",
        "\n",
        "Đối với các dự đoán của mô hình Regularized Greedy Forest được quy định cho 6 bẫy gần nhất, 11 ngày trước đó và 7 ngày trong tương lai đã được thêm vào mức trung bình đơn giản, dự đoán ban đầu cho từng Bẫy, Loài, Ngày đã bị xóa khỏi mức trung bình này (chỉ trong mô hình RGF).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HpRUAv3HLxxD"
      },
      "source": [
        "## Nhìn lại quá trình làm đồ án"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP_Z9DoRL2UR"
      },
      "source": [
        "Sau bao ngày vất vả làm đồ án thì bây giờ đã kết thúc. Bây giờ là lúc để ngồi uống coffee và tĩnh tâm nhìn lại quá trình làm.\n",
        "\n",
        "- Mỗi thành viên: Đã gặp những khó khăn gì? (Hay mọi chuyện đều thuận lợi)\n",
        "- Mỗi thành viên: Có học được gì hữu ích? (Hay không học được gì)\n",
        "- Nhóm: Nếu có thêm thời gian thì sẽ làm gì?\n",
        "\n",
        "Phần này có sao thì bạn nói vậy thôi, chứ không phải là viết cho có, hoặc tự chế ra để nghe cho hay.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r2gg8nTL3Aj"
      },
      "source": [
        "## Tài liệu tham khảo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1QGBQhqL9Vb"
      },
      "source": [
        "Để hoàn thành đồ án này, nhóm bạn đã tham khảo những tài liệu nào?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SOURCE CODE"
      ],
      "metadata": {
        "id": "2d3kS7UfFhYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cài đặt các module cần thiết"
      ],
      "metadata": {
        "id": "omxrc8jWRU59"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Krv0xbGASbz6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9f6222a-d0be-4887-8ec3-9e0b6a85e0f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install sklearn > /dev/null\n",
        "!pip install ml_metrics > /dev/null\n",
        "!pip install metrics > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Upload dataset"
      ],
      "metadata": {
        "id": "sI-d2n_PLGxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/thainguyen222/KHDLUD_NHOM1.git\n",
        "!cp KHDLUD_NHOM1/custom_metrics.py /usr/lib/python3.7/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwBVyFlnWB12",
        "outputId": "421b03d9-a747-4660-b4b2-99819767f7d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'KHDLUD_NHOM1'...\n",
            "remote: Enumerating objects: 59, done.\u001b[K\n",
            "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
            "remote: Compressing objects: 100% (53/53), done.\u001b[K\n",
            "remote: Total 59 (delta 19), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (59/59), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import các library"
      ],
      "metadata": {
        "id": "nPKiHkSBRZ3g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2Cl9LFWPSG7N"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import sklearn\n",
        "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier, \\\n",
        "    RandomForestRegressor, RandomForestClassifier, ExtraTreesClassifier, ExtraTreesRegressor\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import sklearn.metrics as sk_metrics\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import timeit\n",
        "import numpy as np\n",
        "import ml_metrics\n",
        "import ast\n",
        "import itertools\n",
        "import os\n",
        "import math\n",
        "import random\n",
        "from sklearn.pipeline import Pipeline\n",
        "import six\n",
        "import inspect\n",
        "import pickle\n",
        "from __future__ import division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import custom_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FUNCTIONS"
      ],
      "metadata": {
        "id": "WaHQqUCg2vpD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# open file để read \n",
        "def load_model_bin(model_file):\n",
        "    model_file = open(model_file, 'rb')\n",
        "    return pickle.load(model_file)\n",
        "\n",
        "# open file để write\n",
        "def save_model_bin(model, model_file):\n",
        "    model_file = open(model_file, 'wb')\n",
        "    pickle.dump(model, model_file, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "# đọc file csv, được sử dụng để đọc tập train và tập test\n",
        "def load_pd_df(file_name, del_old=False, bin_suffix='.bin.pkl'):\n",
        "    ret_val = None\n",
        "    bin_file_name = file_name + bin_suffix\n",
        "    if os.path.isfile(bin_file_name):\n",
        "        if not os.path.isfile(file_name) or os.path.getmtime(bin_file_name) > os.path.getmtime(file_name):\n",
        "            ret_val = load_model_bin(model_file=bin_file_name)\n",
        "            print(\"Loading %s cache file\" % bin_file_name)\n",
        "\n",
        "    if ret_val is None:\n",
        "        print(\"Loading %s raw file\" % file_name)\n",
        "        ret_val = pd.read_csv(file_name)\n",
        "        print(\"Saving %s cache file\" % bin_file_name)\n",
        "        save_model_bin(model=ret_val, model_file=bin_file_name)\n",
        "        if del_old:\n",
        "            print(\"Erasing %s raw file\" % file_name)\n",
        "            os.remove(file_name)\n",
        "\n",
        "    return ret_val\n",
        "\n",
        "# GradientBoostingRegressor\n",
        "def staged_pred_continuous(model, x):\n",
        "    for pred in model.staged_predict(x):\n",
        "        yield to_2dim(pred)\n",
        "\n",
        "# RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
        "def continuous_predict(model, x):\n",
        "    # [0.10687544 0.10646324 0.10662372 ... 0.10682243]\n",
        "    # [[0.10687544] [0.10646324] [0.10662372] ... [0.10682243]]\n",
        "    return to_2dim(model.predict(X=x))\n",
        " \n",
        "# trả về ma trận 2 chiều từ mảng \n",
        "def to_2dim(array_val):\n",
        "    return np.array(array_val, ndmin=2).transpose()\n",
        "\n",
        "#--------------------------------------------------------------------------------\n",
        "# lấy từ phần tử thứ n \n",
        "def nth(iterable, n):\n",
        "    return next(itertools.islice(iterable, n, None))\n",
        "\n",
        "# Hàm hỗ trợ model: GradientBoostingClassifier\n",
        "def staged_pred_proba(model, x):\n",
        "    for pred in model.staged_predict_proba(x):\n",
        "        yield prob_pred(pred)\n",
        "\n",
        "# trả về xác suất đầu ra tương ứng với giá trị x\n",
        "def pred_proba(model, x):\n",
        "    return prob_pred(model.predict_proba(X=x))\n",
        "\n",
        "# return itself\n",
        "def prob_pred(pred):\n",
        "    return pred\n",
        "\n",
        "# lấy thể hiện của 1 lớp với tên lớp cụ thể, tên lớp được truyền vào là 1 tham số kiểu string\n",
        "def get_class(kls):\n",
        "    parts = kls.split('.')\n",
        "    module = \".\".join(parts[:-1])\n",
        "    m = __import__(module)\n",
        "    for comp in parts[1:]:\n",
        "        m = getattr(m, comp)\n",
        "    return m\n",
        "\n"
      ],
      "metadata": {
        "id": "YW7RIN4P05Sg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PREDICT_TEST()"
      ],
      "metadata": {
        "id": "IEFpVXbjYWdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm dữ đoán tập test dựa trên model đã fit với tập train \n",
        "def predict_test(feat_importance_fun, mappings, model, na_fill_value, predict, staged_predict,\n",
        "                 target_col, test_data_file, test_metric, test_pred_file, x_cols):\n",
        "\n",
        "# Bước 1: Tiền xử lí tập test \n",
        "    # Tập test được đọc từ file test_data_file\n",
        "    test_x = load_pd_df(test_data_file)\n",
        "    test_x = test_x[x_cols]\n",
        "\n",
        "    # Tiền xử lí tập test \n",
        "    for col in test_x.columns:\n",
        "        if col in mappings:\n",
        "              test_x[col] = test_x[col].map(mappings[col]).fillna(na_fill_value)\n",
        "        else:\n",
        "              test_x[col] = test_x[col].fillna(na_fill_value)\n",
        "    \n",
        "    # Cột cần dự đoán: WnvPresent\n",
        "    test_y = None\n",
        "  \n",
        "# Bước 2: Chạy mô hình dự đoán trên tập test \n",
        "    # tập prediction sau khi dùng mô hình dự đoán cho tập test_x\n",
        "    test_pred = predict(model, test_x)\n",
        "\n",
        "# Bước 3: xứ lí tập test_pred \n",
        "    # Tạo dataframe từ tập test_pred\n",
        "    if test_pred.shape[1] == 1:\n",
        "        test_pred = pd.DataFrame({'pred': test_pred[:, 0]})\n",
        "    elif test_pred.shape[1] == 2:\n",
        "        test_pred = pd.DataFrame({'pred': test_pred[:, 1]})\n",
        "    else:\n",
        "        test_pred_df = None\n",
        "        for c in range(test_pred.shape[1]):\n",
        "            if test_pred_df is None:\n",
        "                test_pred_df = pd.DataFrame({'pred0': test_pred[:, c]})\n",
        "            else:\n",
        "                test_pred_df['pred' + str(c)] = test_pred[:, c]\n",
        "        test_pred = test_pred_df\n",
        "\n",
        "    test_pred.to_csv(test_pred_file, index=False)\n",
        "    display(test_pred)\n"
      ],
      "metadata": {
        "id": "Vq0Y1R21YJPl"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRAIN_AND_PREDICT()"
      ],
      "metadata": {
        "id": "id7ZxMeMYfoW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CmP6d3TC3R2f"
      },
      "outputs": [],
      "source": [
        "#train va predict \n",
        "def train_and_predict(train_data_file, test_data_file, target_col, test_pred_file,\n",
        "                      model_type, fit_args, test_metric, na_fill_value, weight_col):\n",
        "\n",
        "# BƯỚC 1: Đọc và tiền xử lí dữ liệu \n",
        "    # train_x là tập train được upload (pandas.DataFrame)\n",
        "    train_x = load_pd_df(train_data_file)\n",
        "\n",
        "    # Tiền xử lí tập train: chuyển đổi chuỗi thành số nguyên \n",
        "    display(train_x.head(2))\n",
        "    mappings = dict()\n",
        "    # data_all là tập chứa mọi giá trị có thể trong tập train + tập test\n",
        "    data_all = train_x.append(load_pd_df(test_data_file))\n",
        "    for col in train_x.columns:\n",
        "        if col not in ['target_col']:\n",
        "            # Xét những cột có datatype là string\n",
        "            if data_all[col].dtype == np.dtype('object'):\n",
        "                 # điền giá trị thiếu và lấy tập các giá trị không trùng nhau của cột  \n",
        "                 s = np.unique(data_all[col].fillna(na_fill_value).values)\n",
        "                 # ánh xạ tập đó vào tập các số nguyên bằng hàm enumerate\n",
        "                 mappings[col] = pd.Series([x[0] for x in enumerate(s)], index=s)\n",
        "                 # ánh xạ từng giá trị trong tập train vào bộ số nguyên này, sau đó điền giá trị thiếu\n",
        "                 train_x[col] = train_x[col].map(mappings[col]).fillna(na_fill_value)\n",
        "            # Nếu cột có datatype là số rồi thì chỉ cần điền giá trị thiếu thôi \n",
        "            else:\n",
        "                 train_x[col] = train_x[col].fillna(na_fill_value)\n",
        "    display(train_x.head(2))\n",
        "    # Xong việc thì xóa data_all\n",
        "    del data_all\n",
        "    \n",
        "    # Tách cột cần dữ đoán ra khỏi tập train (WnvPresent)\n",
        "    train_y = train_x[target_col]\n",
        "    del train_x[target_col]\n",
        "\n",
        "    # extra_fit_args là giá trị tham số phụ để chạy mô hình \n",
        "    # Tách cột NumMosquitos ra khỏi tập train \n",
        "    extra_fit_args = dict()\n",
        "    extra_fit_args['sample_weight'] = train_x[weight_col].values\n",
        "    del train_x[weight_col]\n",
        "\n",
        "    # x_cols là tập các cột của tập train sau khi xóa đi 2 cột: WnvPresent, NumMosquitos\n",
        "    x_cols = train_x.columns\n",
        "\n",
        "    # định nghĩa mặc định các lambda cho các mô hình ???\n",
        "    # Với các model thì sẽ định nghĩa lại một số lambda \n",
        "    # Độ qua trọng của từng đặc trưng\n",
        "    feat_importance_fun = lambda fitted_model: fitted_model.feature_importances_\n",
        "    # hàm dự đoán với tập test \n",
        "    predict = lambda fitted_model, pred_x: fitted_model.predict(pred_x)\n",
        "    staged_predict = lambda fitted_model, pred_x: [predict((fitted_model, pred_x))]\n",
        "\n",
        "# BƯỚC 2: Tạo và chạy mô hình \n",
        "    if model_type == \"RandomForestRegressor\":\n",
        "        model = RandomForestRegressor(**fit_args)\n",
        "        model.fit(X=train_x, y=train_y, **extra_fit_args)\n",
        "        predict = lambda fitted_model, pred_x: continuous_predict(model=fitted_model, x=pred_x)\n",
        "\n",
        "    elif model_type == \"RandomForestClassifier\":\n",
        "        model = RandomForestClassifier(**fit_args)\n",
        "        model.fit(X=train_x, y=train_y, **extra_fit_args)\n",
        "        predict = lambda fitted_model, pred_x: pred_proba(model=fitted_model, x=pred_x)\n",
        "        staged_predict = lambda fitted_model, pred_x: [predict((fitted_model, pred_x))]\n",
        "\n",
        "    elif model_type == \"ExtraTreesRegressor\":\n",
        "        model = ExtraTreesRegressor(**fit_args)\n",
        "        model.fit(X=train_x, y=train_y, **extra_fit_args)\n",
        "        predict = lambda fitted_model, pred_x: continuous_predict(model=fitted_model, x=pred_x)\n",
        "\n",
        "    elif model_type == \"ExtraTreesClassifier\":\n",
        "        model = ExtraTreesClassifier(**fit_args)\n",
        "        model.fit(X=train_x, y=train_y, **extra_fit_args)\n",
        "        predict = lambda fitted_model, pred_x: pred_proba(model=fitted_model, x=pred_x)\n",
        "        staged_predict = lambda fitted_model, pred_x: [predict((fitted_model, pred_x))]\n",
        "\n",
        "    elif model_type == \"GradientBoostingRegressor\":\n",
        "        # tạo mô hình với bộ tham số truyền vào \n",
        "        model = GradientBoostingRegressor(**fit_args)\n",
        "        # chạy mô hình này trên tập train với tham số phụ là cột NumMosquitos\n",
        "        model.fit(X=train_x, y=train_y, **extra_fit_args)\n",
        "        # định nghĩa lại các lambda \n",
        "        predict = lambda fitted_model, pred_x: continuous_predict(model=fitted_model, x=pred_x)\n",
        "        staged_predict = lambda fitted_model, pred_x: staged_pred_continuous(model=fitted_model, x=pred_x)\n",
        "        \n",
        "    elif model_type == \"GradientBoostingClassifier\":\n",
        "        model = GradientBoostingClassifier(**fit_args)\n",
        "        model.fit(X=train_x, y=train_y, **extra_fit_args)\n",
        "        staged_predict = lambda fitted_model, pred_x: staged_pred_proba(model=fitted_model, x=pred_x)\n",
        "        predict = lambda fitted_model, pred_x: pred_proba(model=fitted_model, x=pred_x)\n",
        "        \n",
        "    elif model_type == \"LogisticRegression\":\n",
        "        model = LogisticRegression(**fit_args)\n",
        "        model.fit(X=train_x, y=train_y)\n",
        "        predict = lambda fitted_model, pred_x: pred_proba(model=fitted_model, x=pred_x)\n",
        "        staged_predict = lambda fitted_model, pred_x: [predict((fitted_model, pred_x))]\n",
        "        feat_importance_fun = lambda fitted_model: None\n",
        "\n",
        "    elif model_type == \"SVC\":\n",
        "        model = sklearn.svm.SVC(**fit_args)\n",
        "        model.fit(X=train_x, y=train_y)\n",
        "        predict = lambda fitted_model, pred_x: pred_proba(model=fitted_model, x=pred_x)\n",
        "        staged_predict = lambda fitted_model, pred_x: [predict((fitted_model, pred_x))]\n",
        "        feat_importance_fun = lambda fitted_model: None\n",
        "\n",
        "    elif model_type == \"Pipeline\":\n",
        "        model = Pipeline([\n",
        "            ('pre_process', get_class(fit_args['pre_process']['name'])(**fit_args['pre_process']['args'])),\n",
        "            ('model', get_class(fit_args['model']['name'])(**fit_args['model']['args']))\n",
        "        ])\n",
        "        model.fit(X=train_x, y=train_y)\n",
        "        predict = lambda fitted_model, pred_x: pred_proba(model=fitted_model, x=pred_x)\n",
        "        staged_predict = lambda fitted_model, pred_x: [predict((fitted_model, pred_x))]\n",
        "        feat_importance_fun = lambda fitted_model: None\n",
        "\n",
        "    del train_x, train_y\n",
        "\n",
        "# BƯỚC 3: Dự đoán tập test với mô hình đã chạy ở trên \n",
        "    predict_test(feat_importance_fun=feat_importance_fun,\n",
        "                 model=model,                                # model: GradientBoostingRegressor\n",
        "                 mappings=mappings,                          # mappings: tập ánh xạ các giá trị \n",
        "                 na_fill_value=na_fill_value,                # na_fill_value: -20000\n",
        "                 predict=predict,                            # lambda: predict \n",
        "                 staged_predict=staged_predict,              # lambda: staged_predict\n",
        "                 target_col=target_col,                      # target_col: WnvPresent\n",
        "                 test_data_file=test_data_file,              # file tập test \n",
        "                 test_metric=test_metric,                    # test_metric: normalized_weighted_gini\n",
        "                 test_pred_file=test_pred_file,              # prediction file \n",
        "                 x_cols=x_cols)                              # tập các cột của file train \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MAIN()"
      ],
      "metadata": {
        "id": "hG-U6g7kR5hR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    #truyen cac tham so cho bai toan \n",
        "    args =\t{\n",
        "       \"train_data_file\": \"/content/KHDLUD_NHOM1/train.csv\",     # đường dẫn file tập train        \n",
        "       \"test_data_file\": \"/content/KHDLUD_NHOM1/test.csv\",       # đường đãn file tập test \n",
        "       \"test_pred_file\": \"/content/KHDLUD_NHOM1/test_pred.csv\",  # đường dẫn file prediction của tập test \n",
        "       \"test_metric\": \"normalized_weighted_gini\",                # độ đo dánh giá \n",
        "       \"target_col\": \"WnvPresent\",                               # cột cần dự đoán trong tập test \n",
        "       \"weight_col\": \"NumMosquitos\",\n",
        "       \"model_type\": \"GradientBoostingRegressor\",                # model \n",
        "       #\"model_type\": \"GradientBoostingClassifier\",  \n",
        "       \"na_fill_value\": -20000,                                  # giá trị điền vào chỗ thiếu    \n",
        "       \"fit_args\": '{\\\"n_estimators\\\": 10,  \\\"learning_rate\\\": 0.001, \\\"loss\\\": \\\"ls\\\",  ' # tham số cho model \n",
        "                    '\\\"max_features\\\": 5, \\\"max_depth\\\": 7,  \\\"random_state\\\": 788954,  '\n",
        "                    '\\\"subsample\\\": 1, \\\"verbose\\\": 50}'\n",
        "    }\n",
        "    \n",
        "    #deviance\n",
        "    # tiền xử lí tham số cho model \n",
        "    args['fit_args'] = ast.literal_eval(args['fit_args'])\n",
        "    for key in args['fit_args']:\n",
        "        if isinstance(args['fit_args'][key], six.string_types):\n",
        "            if args['fit_args'][key] in args:\n",
        "                args['fit_args'][key] = args[args['fit_args'][key]]\n",
        "\n",
        "    # train va predict \n",
        "    train_and_predict(**args)"
      ],
      "metadata": {
        "id": "incNsa7m-vOg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Report_KHDLUD_NHOM1.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}